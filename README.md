# Shoten

[![Python package](https://img.shields.io/pypi/v/shoten.svg)](https://pypi.python.org/pypi/shoten)
[![Python versions](https://img.shields.io/pypi/pyversions/shoten.svg)](https://pypi.python.org/pypi/shoten)
[![Code Coverage](https://img.shields.io/codecov/c/github/adbar/shoten.svg)](https://codecov.io/gh/adbar/shoten)
[![image](https://zenodo.org/badge/DOI/10.5281/zenodo.6365800.svg)](https://doi.org/10.5281/zenodo.6365800)

Helper functions to find word trends (i.e. extract tokens, lemmatize and filter).


## Installation

With `pip` (`pip3` where applicable):

- Package repository: `pip install shoten`
- Latest development version:
  `pip install -U git+https://github.com/adbar/shoten.git`


## Usage

### Input

Two possibilities for input data:

- XML-TEI files as generated by [trafilatura](https://trafilatura.readthedocs.io/):
   1. `from shoten import gen_wordlist`
   2. `vocab = gen_wordlist(mydir, ['de', 'en'])`

- TSV-file contaning a word list: word form + `TAB` + date (`YYYY-MM-DD` format) + possible 3rd column (source)
   1. `from shoten import load_wordlist`
   2. `vocab = load_wordlist(myfile, ['de', 'en'])`

Language codes: optional list of languages to be considered for
lemmatization, ordered by relevance. [ISO 639-1
codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes), see the
[list of supported languages](https://github.com/adbar/simplemma).

Optional argument `maxdiff`: maximum number of days to consider
(default: 1000, i.e. going back up to 1000 days from today).

### Filters

`from shoten.filters import *`

Given a vocabulary `vocab` as explained above:

- `hapax_filter(vocab, freqcount=2)`: (default frequency: \<= 2)
- `different_days_filter(vocab, threshold=2)`: word seen on at least n
  days
- `regex_filter(vocab, string)`: custom regular expression
- `compound_filter(vocab, language, threshold)`: examine parts of
  hyphenated compounds based on target language and corpus frequency
- `shortness_filter(vocab, threshold=20)`: length threshold in percent
  of word lengths
- `frequency_filter(vocab, max_perc=50, min_perc=.001)`: maximum and
  minimum frequencies in percent
- `oldest_filter(vocab, threshold=50)`: discard the oldest words
  (threshold in percent)
- `freshness_filter(vocab, percentage=10)`: keep the X% freshest words
- `ngram_filter(vocab, threshold=90, verbose=False)`: retains X% words
  based on character n-gram frequencies; runs out of memory if the
  vocabulary is too large (8 GB RAM recommended)
- `sources_freqfilter(vocab, threshold=2)`: remove words which are
  only present in less than x sources
- `sources_filter(vocab, myset)`: only keep the words for which the
  source contains a string listed in the input set
- `wordlist_filter(vocab, mylist, keep_words=False)`: keep or discard
  words present in the input list

Reduce vocabulary size with a filter:

`vocab = oldest_filter(vocab)`

Filter chains:

`vocab = oldest_filter(shortness_filter(vocab))`

### Output

``` python
# print one-by-one
for word in sorted(vocab):
    print(word)
# transfer to a list
results = [w for w in vocab]
```

### CLI

`shoten --help`

## Additional information

*Shoten* = focal point in Japanese
([焦点](https://en.wiktionary.org/wiki/%E7%84%A6%E7%82%B9#Japanese)).

Project webpage: [Webmonitor](https://www.dwds.de/d/korpora/webmonitor).
